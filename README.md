# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
This dataset contains data about is used for markering to predict if a client will subscribe to a bank term desposit

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best performing model was a Ensemble algorithm, Voting Ensemble, proposed by AutoML

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
Hyperparemters are optimized against the primary metric, in this case Accurracy. A search space is defined for each hyperparameter by chosing a parameter sampling method. The data is a classification problem
where the user is either a potential subsriber or not. As such a classification algorithm is used in the training script; LogisticRegression. The pipeline tries several parallel runs to tune the hyperparameters and come up with the
best model.

**What are the benefits of the parameter sampler you chose?**
RandomParameterSampling has the advantage for early termination by low performance runs. If a more throughout result is required you can then refine the search to improve results.

**What are the benefits of the early stopping policy you chose?**
Banditpolicy makes sure to stop the run if the primary metric is not within the slack factor/amount comparted to the best performing run. In my notebook I went by slack factor.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
AutoML genereated a Voting Ensemblem model, which by soft voting determine the weights aswell as hyperparameters of the base classififers.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
The accuracy for the AutoML model ran to 0.91680
The accuracy for the HyperDrive model ran for 0.91420 with a --C of 0.1 and a --max_iter of 100

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
We can run a more refined search for the hyperdrive with another parameter sampler. The hyperdrive pipeline could also increase its maximum amount of runs and include more hyperparameters.
For the AutoML run there is the possibility to increase the amount of n cross validations or try another priamry metric to decide a better model.